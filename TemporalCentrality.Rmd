---
title: "Network-Based Prediction of Infection Susceptibility from Contact Tracing Data"
author: "Nicholas Winsley"
date: "2025-03-19"
output:
  pdf_document:
    toc: true
  html_document:
    toc: true
    df_print: paged
bibliography: references.bib
linestretch: 1.5
fontsize: 12pt
header-includes:
- \usepackage{tikz}
- \usepackage{pgfplots}
- \usepackage[linesnumbered,ruled,longend,vlined]{algorithm2e}
- \usepackage{pifont}
- \DeclareUnicodeCharacter{2713}{\checkmark}
- \DeclareUnicodeCharacter{2717}{\ding{55}}
- \usepackage{float}
---

```{r setup, include = FALSE}
knitr::opts_knit$set(root.dir = getwd())
```

# Introduction

The recent Covid-19 pandemic highlighted the importance of effective methods for combating the spread of disease. With the emergence of automated contact tracing technology, prophylactic identification and isolation of high-risk individuals could be practicable in the future. This study investigates methods for the identification of high-risk individuals in a temporal network.

Government's intervene for individuals who are most instrumental in the spread of a disease. Many factors may influence this decision, such as medical history, clinical knowledge and past a future social contacts. Future contacts are typically not known, so this study will primarily focus on past contacts.

Social networks can be modeled as a number of close personal contacts, each paired with a proximity measurement. Each contact is written as (i, j, t), where i and j are the interacting individuals, and t is the time of the interaction (note that (i, j, t) is semantically no different from (j, i, t)). This representation is called a temporal network, and it is frequently used in the study of epidemics. In practice, contact events are identified by contact tracing methods.

Contact tracing is a methodology for combating the spread of infectious diseases transmitted by close personal contacts. By uncovering close contact events, contact tracing can be used to identify people at high risk of infection, and foresee future growth or contraction of the epidemic. This information can inform further interventions (e.g. quarantining, disease tests, vaccination). Traditionally, contact tracing focused on confirmed cases, which were reported to authorities. When a new case is reported, an official asks the patient to recall all recent contacts. Although this method has been shown to be effective (@fetzer2021measuring), it is labor-intensive and does not give a complete picture. Recently, digital contact tracing has emerged as a cost-effective (albeit unreliable) alternative to conventional contact tracing methods. In it's latest versions, digital contact tracing uses portable bluetooth devices which detect close contacts between carriers of the device. Digital contact tracing can provide additional proximity indicators for contacts, which may be used to estimate risk. Statistical methods are often used to identify high-risk individuals in a social network. This study investigates two common methods for the analysis of contact tracing data: Simulation and Social Network Analysis (SNA).

Social Network Analysis (SNA) is an interdisciplinary approach for the study of entities and their relationships with each other. SNA uses a network to model a real-world situation of interest. SNA metrics can be categorized into two types: Population-level measures and Individual-level measures. This study is primarily concerned with individual-level measures. Centrality is a common individual-level measure which is defined as the influence of a particular individual within a network. Prestige (sometimes called status or rank) is a similar individual-level measure for directional graphs, which only considers incoming dyads. Although SNA has been used in many fields since it's creation in the 1930s, it's value in epidemiology only became apparent in 1985, when @klovdahl1985social applied SNA to AIDS data.

Simulation is an effective method for ascertaining properties of a temporal network. Some classical models have been deterministic: the differential equation model of @kermack1927contribution is a notable example. However, deterministic models typically rely on simplifying assumptions, and thus do not give the full picture. Compartmental models are a popular simulation approach in which the population is divided into groups, and individuals transition between groups over time. The Susceptible-Infected-Recovered (SIR) model is a common compartmental model in which all individuals are initially susceptible, and individuals may become infected due to a contact with a contagious individual. Once infected, individuals transition to the recovered state, where they remain for the rest of the simulation. Recovery times (time from infection to recovery) are typically sampled from a probability distribution, which may be estimable by exogenous information (e.g. medical knowledge, recovery rates for similar diseases). Key metrics are averaged over many simulations to approximate a true underlying distribution. A wide array of summary metrics have been applied to simulated epidemics. @macdonald1952analysis introduced the reproductive number, which is defined as the number of cases resulting from a single infection. @holme2018objective used the time for the disease to go extinct (no new cases can occur).

This study aims to address several key research questions:

1. Can we make simplifying assumptions to reduce computation time of simulations?

2. Which centrality measures are most effective when applied to epidemiology?

3. How can we extend these measures to cases where contact risk varies?


# Data

## Background

In 2020, the New Zealand Ministry of Health (MoH) commissioned a pilot study of the "CovidCard", a portable device which uses bluetooth technology to record contacts between carriers of the device. Adults 19 years of age or older who live in Ngontotahā West and East were recruited to participate in a seven-day study. Additionally, people who live outside these boundaries but work within the Ngontotahā Village were also permitted to take part in the trial. Ngontotahā was chosen because it met several key criteria, namely compactness, geographical isolation, small population size and high sociodemographic diversity. In total, 1,191 people participated in the study. At the end of the trial period, a subset of 158 participants from the main trial were contacted by MoH case investigators to establish contacts that they had over the trial period using a modified version of the MoH case investigation protocol. The study compared the CovidCard to conventional case investigation methods and found a greater proportion of reciprocal interactions identified by the CovidCard, indicating that the CovidCard is an effective contact tracing method. We use data obtained via the CovidCard to compare and validate existing centrality metrics, and develop novel alternatives.

## Data Description

The CovidCard is a bluetooth device developed for detecting close-contact events between carriers. Each card advertises it's presence and detects signals from other cards. Algorithms evaluate the radio signal strength indicator (RSSI) of close-contacts in real-time, and the signal strength is aggregated over 15 minute time intervals. The raw RSSI values are transformed into distance estimates by the path loss model, proposed by @seidel1992914, for which

$$RSSI \propto -20\log_{10}(distance)$$
Noise in distance estimates is subsequently reduced by signal processing methods, most commonly Kalman Filters.

Each interval was classified as either < 1 meter, < 2 meter and < 4 meter proximity, and the total number of intervals belonging to each class was summed over a two-hour period. The cards can hold up to 128 contact events in short-term cache memory at any given time, of which some are recorded in long-term flash memory. An interaction was recorded in flash memory if it was longer than 2 minutes in duration, and the RSSI exceeded -62dBm (roughly corresponding to a distance of less than 4 meters). For more details on the CovidCard, see @admiraal2022case.

## Data Preparation

The first and last days of the trial period saw an anomalously high number of close-contacts, most likely because participants congregated at a single location for card collection. For this reason, contact events which occurred on the first and last days of the trial were omitted. Participants who could not be cross-verified by case investigation were removed. If two cards registered the same contact event, and gave conflicting proximity values, one of the proximity values was arbitrarily removed. Contact dates were converted to numeric times by calculating the time elapsed (in hours) between the contact date and the start of the trial. Some cards were collected before the last day of the trial, resulting in an anomalous number of contacts during card collection. For this reason, all contact events which occurred on the day in which they were uploaded were removed. Data on the proximity classes was processed to form non-overlapping categories. For instance, the number of 15-intervals with a distance less than 4 metres, $n_{< 4}$, was transformed by subtracting $n_{< 2}$ and $n_{< 1}$ to get $n_{\geq 3, < 4}$; the total number of 15-minute intervals between 3 and 4 metres. By doing this, we get a categorical variable on which further statistical models are based. When the same contact was recorded by two cards and the RSSI indicators differed between cards, the information from either card was arbitrarily chosen. In practice, a weighted average of RSSI indicators from both cards may be used.


# Methods

## Estimation of Transmission Probability

Probabilities were estimated by using proximity categories as predictors in a generalized linear model. The number of 15-time intervals spent in each class is given over the two hour observation period. Transmission probabilities were then calculated by a logistic regression model of the form:

\begin{equation}
\label{eq:logistic}
\log\left(\frac{\pi(x)}{(1 - \pi(x))}\right) = \alpha + \beta_{1}c_{1} + \beta_{2}c_{2} + \beta_{3}c_{3}
\end{equation}

where $\pi(x)$ is the transmission probability of the contact and $c_{i}$ is the number of 15-minute time intervals spent in class i during the two hour observation period. The classes are indexed in increasing order of proximity (i.e. class 3 indicates greater proximity than class 2), therefore the coefficients were chosen to satisfy the constraint $\beta_{3} > \beta_{2} > \beta_{1}$. One may treat the classes as levels of an ordinal variable and assume a proportional relationship, which reduces the model to:

$$\log\left(\frac{\pi(x)}{(1 - \pi(x))}\right) = \alpha + \beta_{1}(c_{1} + 2c_{2} + 3c_{3})$$

However, this simplified model was rejected due to insufficient evidence to suggest a proportional relationship. In practice, transmission probabilities can be estimated by exogenous information (for instance, clinical knowledge or self-reported infections paired with contact tracing data). Table 1 shows the coefficient values used for all subsequent simulations.

**Table 1**

\begin{center}
\begin{tabular}{l|l|l|l}

$\alpha$ & $\beta_{1}$ & $\beta_{2}$ & $\beta_{3}$ \\

\hline

-4 & 0.1 & 0.2 & 0.4 \\

\end{tabular}
\end{center}

## Centrality

Centrality is roughly defined as the importance of a given node in a graph. The meaning of a "central" node depends on the context. We discuss several centrality metrics and their use cases.

## Degree Centrality

Degree centrality is a simple metric for static networks where we only consider the number of neighbors of a given node. Let A be the adjacency matrix for a static network i.e. $A_{ij}$ = 1 if i and j are neighbours and 0 otherwise. The degree centrality is defined as:

\begin{equation}
\label{eq:degree}
C_{D}(i) = \sum_{j = 1}^{N}{A_{ij}}
\end{equation}

Degree centrality is simple and easy to calculate, however it does not incorporate knowledge of the entire network structure. Improving this point is the motivation for our next centrality measure.

## Closeness Centrality

For undirected graphs, @bavelas1950communication proposed closeness centrality

\begin{equation}
\label{eq:close}
C_{c}(\textit{u}) = \frac{N - 1}{\sum_{\upsilon \neq \textit{u}}d(\textit{u}, \upsilon)}
\end{equation}

where N is the number of nodes and $d(\textit{u}, \upsilon)$ is the distance of the shortest path between $\upsilon$ and $\textit{u}$. Thus, referring to Figure 1, the closeness centrality of node E is $\frac{4}{1 + 1 + 2 + 3} = \frac{4}{7}$. Closeness centrality can be roughly interpreted as the efficiency of reaching all other nodes in a network.

```{r, engine = "tikz", fig.cap = "A simple undirected graph", echo = FALSE}
\begin{tikzpicture}

\tikzset{vertex/.style = {shape=circle,draw,minimum size=1.5em}}
\tikzset{edge/.style = {->,> = latex}}

\node[vertex] (a1) at (0,-1) {$A$};
\node[vertex] (b1) at (0.5,1) {$B$};
\node[vertex] (c1) at (3,3) {$C$};
\node[vertex] (d1) at (-1,5) {$D$};
\node[vertex] (e1) at (3,7) {$E$};

\draw[thick] (a1) -- (b1);
\draw[thick] (a1) -- (c1);
\draw[thick] (a1) -- (d1);
\draw[thick] (e1) -- (c1);
\draw[thick] (d1) -- (e1);

\end{tikzpicture}
```

## Betweenness Centrality

In some situations, the effect of removing a node on transmission through a network may be highly important (for instance, when quarantining individuals during a pandemic). This is the primary motivation behind Betweenness Centrality (@freeman1977set), which is defined as

\begin{equation}
  \label{eq:between}
  C_{B}(\upsilon) = \frac{1}{(N - 1)(N - 2)}\sum_{s \neq v \neq r}\frac{\sigma_{s, r}(\upsilon)}{\sigma_{s, r}}
\end{equation}

where $\sigma_{s, r}$ is the number of shortest paths (geodesics) between s and r, and $\sigma_{s, r}(\upsilon)$ is the number of such paths that pass through $\upsilon$. The denominator term, $(N - 1)(N - 2)$, ensures that the value is normalized between 0 and 1.

## Percolation Centrality

In practice, additional information around the percolation state of nodes may be known. For instance, in epidemiology we may know that certain individuals are infected. To incorporate knowledge of percolation state into centrality metrics, @piraveenan2013percolation proposed percolation centrality:

\begin{equation}
  \label{eq:perc}
  C_{P}^{t}(\upsilon) = \frac{1}{(N - 1)(N - 2)}\sum_{s \neq v \neq r}\frac{\sigma_{s, r}(\upsilon)}{\sigma_{s, r}}\frac{x_{s}^{t}}{[\sum{x_{i}^{t}}] - x_{\upsilon}^{t}}
\end{equation}

where $x_{i}^{t}$ is the percolation state of node i at time t. The percolation state ranges from 0 to 1, where 1 means the individual is certainly infected, and 0 means the individual is healthy. A decimal value (say 0.6) could, for instance, represent a probability of infection or a proportion of a township which is infected.

## Adjusted Percolation Centrality

I propose a novel variant of percolation centrality, which I will call Adjusted Percolation Centrality. Adjusted Percolation Centrality only considers paths which do not pass through any percolated nodes. By doing this, it ensures that redundant paths are not considered. We will define an unpercolated path as a path where no incident nodes are percolated, except for the start and end nodes, which may have any percolation state. The mathematical definition is


\begin{equation}
  \label{eq:adj}
  C_{P}^{t}(\upsilon) = \frac{1}{M_{P}}\sum_{s \neq v \neq r}\frac{\sigma_{s, r}^{P}(\upsilon)}{\sigma_{s, r}^{P}}\frac{x_{s}^{t}}{[\sum{x_{i}^{t}}] - x_{\upsilon}^{t}}
\end{equation}

where $M_{P}$ is the number of pairs (i, j) where there is an unpercolated path between i and j, and $\sigma_{s, r}^{P}$ is the number of shortest unpercolated paths between s and r.

## A Simple Algorithm for Betweenness Centrality

Adjusted percolation centrality was calculated by a modified version of a novel algorithm for calculating betweenness centrality, shown in Algorithm 1.

\begin{algorithm}
\caption{Novel algorithm for calculating Betweenness Centrality}
\KwResult{Betweenness Centrality}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\RestyleAlgo{boxruled}
\DontPrintSemicolon
\Input{Undirected Static Network}
\Output{Betweenness Centrality of all nodes in the network}
\BlankLine

n <- Number of nodes in the network.\;

\BlankLine

nodes <- List of nodes in the network. Each node is represented by a data structure with a list of neighbours and a unique id (number from 1 to n).\;

\BlankLine

queue <- Priority queue of nodes to be processed during breadth search. Each queue item is a data structure with a depth attribute representing the distance from the starting node of the search, and a node id attribute. Nodes are ordered by depth in increasing order, so that the element processed first always has the lowest depth.\;

\BlankLine

dist <- n by n 2D array for storing the shortest path distance between nodes. The array is indexed by node id's.\;

\BlankLine

numPaths <- n by n identity matrix for storing the number of shortest paths between two nodes.\;

\BlankLine

bc <- Vector for storing betweenness centrality. The vector is indexed by node id.\;

\BlankLine

Set all entries in dist to n\;

\BlankLine

\end{algorithm}

\clearpage

\begin{algorithm}
\RestyleAlgo{boxruled}
\DontPrintSemicolon
\ForEach{node in nodes}{
  queue <- Empty priority queue\;
  
  startID <- node.id\;
  
  Add node to queue with a depth of 0\;
  
  \While{queue has next element}{
    node <- element\;
    depth <- element.depth\;
    Remove front element from queue\;
    id <- node.id\;
    dist[startID, id] <- depth\;
    \ForEach{nb in node.neighours}{
      \If{dist[startID, nb.id] == n}{
        dist[startID, nb.id] <- depth + 1\;
        Add nb to queue with newDepth = depth + 1\;
      }
      \ElseIf{dist[startID, nb.id] == depth - 1}{
        numPaths[startID, id] <- numPaths[startID, id] + numPaths[startID, nb.id]\;
      }
    }
  }
  
  \ForEach{node1, node2, node3 in nodes}{
    i <- node1.id\;
    j <- node2.id\;
    k <- node3.id\;
    \If{dist[j, k] == dist[j, i] + dist[i, k]}{
      bc[node1] = bc[node1] + (numPaths[j, i] $\times$ numPaths[i, k])/numPaths[j, k]\;
    }
    \If{dist[i, k] == dist[i, j] + dist[j, k]}{
      bc[node2] = bc[node2] + (numPaths[i, j] $\times$ numPaths[j, k])/numPaths[i, k]\;
    }
    \If{dist[i, j] == dist[i, k] + dist[k, j]}{
      bc[node3] = bc[node3] + (numPaths[i, k] $\times$ numPaths[k, j])/numPaths[i, j]\;
    }
  }
  \Return(bc/((n - 1) $\times$ (n - 2)))\;
}
\end{algorithm}


## Katz Centrality

Betweenness and Closeness centrality focus on shortest paths, which potentially do not give the full picture. Katz centrality (@katz1953new) is an alternative approach which considers all paths. For the i'th node in a network, Katz centrality is defined as:

\begin{equation}
  \label{eq:katz}
  C_{K}(i) = \sum_{j = 1}^{n}{(I + \alpha{A} + \alpha^{2}A^{2} + \alpha^{3}A^{3} + ...)_{ij}}
\end{equation}

where A is the adjacency matrix and $0 <= \alpha < 1$. $A^{n}$ is the n-step adjacency matrix for the network, and $\alpha^{n}$ decreases as n grows, which means that longer paths are down-weighted. For a technical reason, the infinite sum ($I + \alpha{A} + ...$) converges to $(I - \alpha{A})^{-1}$ when $\alpha <= \frac{1}{\rho(A)}$ where $\rho(A)$ is the spectral radius of A (the largest absolute value of any of it's eigenvalues). If we are only interested in the first n steps, we adjust the formula to get $(I - \alpha^{n + 1}A^{n + 1})(I - \alpha{A})^{-1}$. This formula is convenient for temporal networks, where a finite number of steps in each snapshot is often assumed.

## Temporal Katz Centrality

Temporal Katz Centrality, proposed by @grindrod2011communicability, is an extension of Katz centrality to temporal networks. Denote by $A_{t}$ the adjacency matrix for the snapshot at time t. For a temporal network with snapshots at times $1, 2, 3, ..., T - 1, T$, the Temporal Katz Centrality is defined by the matrix product

\begin{equation}
\label{eq:tempkatz}
  (I - \alpha{A_{1}})^{-1}(I - \alpha{A_{2}})^{-1}...(I - \alpha{A_{T - 1}})^{-1}(I - \alpha{A_{T}})^{-1}
\end{equation}

The centrality for a given node is likewise calculated by the row sum of this matrix product.

## Time-Ordered Networks

Generalization of conventional centrality measures to temporal networks requires a high-granularity representation of the network as a graph. @kempe2000connectivity proposed a graph where the edge weights are contact times, however this model fails to account for differential rates of transmission. @kim2012temporal proposed a more general solution using a time-ordered directed graph. Consider a network of N nodes for which M edges are observed over T time points. Without loss of generality, rank the contact times to get a list, $\textit{t} = (0, 1, 2, \cdots, T)$. It should be clear that contact times may have decimal values, though we represent time by discrete integer values to simplify notation. In practice, contact times are continuous random variables, however any sample of contact times can be ranked to get a list of discrete "snapshots" sorted by time. By doing this, we can construct a time-ordered graph where each node appears T + 1 times. Denote by $\upsilon_{t}$ the node $\upsilon$ at time t. In this graph, a directed edge from $\upsilon_{t}$ to $\textit{u}_{t + 1}$ only exists if $\upsilon = \textit{u}$ or there is a contact between $\upsilon$ and $\textit{u}$ at time t. We can construct this graph for any temporal network without loss of information. In practice, computational constraints may require aggregation of contact times and thus loss of information.

To illustrate this idea, consider a simple temporal network with five individuals, as shown in Table 2. 

  
**Table 2**

\begin{table}[!h]
  \begin{tabular}{|c|c|c|}
  \hline
    First Individual & Second Individual & Time of contact \\ \hline
    a & b & 1 \\ \hline
    b & d & 2 \\ \hline
    a & c & 2 \\ \hline
    d & e & 1 \\ \hline
  \end{tabular}
\end{table}

Figures 2 and 3 show snapshots of the network at two time points

```{r, engine = "tikz", echo = FALSE, fig.cap = "Snapshot of the network when t = 1"}
\begin{tikzpicture}

\tikzset{vertex/.style = {shape=circle,draw,minimum size=1.5em}}
\tikzset{edge/.style = {->,> = latex}}

\node[vertex] (a) at (0,0) {a};
\node[vertex] (b) at (0, 2) {b};
\node[vertex] (c) at (2, 0) {c};
\node[vertex] (d) at (2, 2) {d};
\node[vertex] (e) at (1, 1) {e};

\draw[thick] (a) -- (b);
\draw[thick] (d) -- (e);

\end{tikzpicture}
```

```{r, engine = "tikz", echo = FALSE, fig.cap = "Snapshot of the network when t = 2"}
\begin{tikzpicture}

\tikzset{vertex/.style = {shape=circle,draw,minimum size=1.5em}}
\tikzset{edge/.style = {->,> = latex}}

\node[vertex] (a) at (0,0) {a};
\node[vertex] (b) at (0, 2) {b};
\node[vertex] (c) at (2, 0) {c};
\node[vertex] (d) at (2, 2) {d};
\node[vertex] (e) at (1, 1) {e};

\draw[thick] (b) -- (d);
\draw[thick] (a) -- (c);

\end{tikzpicture}
```

Figure 4 shows the temporal network represented as a directed graph.

```{r, engine = "tikz", echo = FALSE, fig.cap = "A simple temporal network represented as a digraph. The temporal shortest path from a to b is shown in red."}
\begin{tikzpicture}

\tikzset{vertex/.style = {shape=circle,draw,minimum size=1.5em}}
\tikzset{edge/.style = {->,> = latex}}

\node[vertex] (a1) at (0,-1) {$a_{1}$};
\node[vertex] (b1) at (0,1) {$b_{1}$};
\node[vertex] (c1) at (0,3) {$c_{1}$};
\node[vertex] (d1) at (0,5) {$d_{1}$};
\node[vertex] (e1) at (0,7) {$e_{1}$};

\draw[dashed] (1,-2)--(1,8);

\node[vertex] (a2) at (2,-1) {$a_{2}$};
\node[vertex] (b2) at (2,1) {$b_{2}$};
\node[vertex] (c2) at (2,3) {$c_{2}$};
\node[vertex] (d2) at (2,5) {$d_{2}$};
\node[vertex] (e2) at (2,7) {$e_{2}$};

\draw[dashed] (3,-2)--(3,8);

\node[vertex] (a3) at (4,-1) {$a_{3}$};
\node[vertex] (b3) at (4,1) {$b_{3}$};
\node[vertex] (c3) at (4,3) {$c_{3}$};
\node[vertex] (d3) at (4,5) {$d_{3}$};
\node[vertex] (e3) at (4,7) {$e_{3}$};

\draw[edge] (a1) to (a2);
\draw[edge] (a2) to (a3);
\draw[edge] (b1) to (b2);
\draw[edge] (b2) to (b3) [red];
\draw[edge] (b1) to (a2);
\draw[edge] (a2) to (c3);
\draw[edge] (c1) to (c2);
\draw[edge] (c2) to (c3);
\draw[edge] (a1) to (b2) [red];
\draw[edge] (c2) to (a3);
\draw[edge] (d1) to (d2);
\draw[edge] (d2) to (d3);
\draw[edge] (e1) to (e2);
\draw[edge] (e2) to (e3);
\draw[edge] (d1) to (e2);
\draw[edge] (e1) to (d2);
\draw[edge] (d2) to (b3);
\draw[edge] (b2) to (d3);

\node at (0,-2) {t = 1};
\node at (2,-2) {t = 2};
\node at (4,-2) {t = 3};

\end{tikzpicture}
```

Define the distance of the temporal shortest path length over time interval [i, j], denoted by $d_{i,j}(\upsilon, \textit{u})$, as the smallest $d = j - n$, where $i \leq n \leq j$ and there is a path from $\upsilon_{n}$ to $\textit{u}_{j}$. Thus, in Figure 1, the shortest path distance $d_{1,3}(a, b)$ is two, with the temporal shortest path being $a_{1}$-$b_{2}$-$b_{3}$. By representing a temporal network as a high-granularity digraph, we can now generalize conventional measures of centrality to temporal networks.


## Temporal Closeness Rank and Temporal Proximity Rank

In a directional network, a node with high rank (sometimes called prestige) is the object of many ties i.e. has many incoming connections. Conversely, a node with high centrality has may incoming outgoing connections. Rank may be used, for instance, to estimate the likelihood of an individual becoming infected during an epidemic. Many conventional centrality measures are undefined in directional graphs, as directional graphs may not be strongly-connected. Consequently, we often consider nodes in the influence domain of node $\upsilon$ (all nodes, $\textit{u}$, where $\upsilon$ is reachable from $\textit{v}$). @lin1976foundations proposed the proximity prestige, a metric for directional graphs which is defined as:

$$P_{p}(\upsilon) = \frac{I_{\upsilon}/(g - 1)}{\sum_{\textit{u} \in I_{\upsilon}}{d(\upsilon, \textit{u})}/I_{i}}$$
where $I_{\upsilon}$ is the size of the influence domain of $\upsilon$, g is the total number of nodes in the network and $d(\upsilon, \textit{u})$ is the shortest path distance. Proximity prestige is the proportion of the network covered by the influence domain, divided by the average temporal distance over the influence domain. When the influence domain is empty, the proximity prestige is defined to be 0.

For temporal networks, I propose a modified version

$$P_{p}(\textit{u}_{i}) = \sum_{t = 0}^{i - 1}\frac{I_{t, i, \textit{u}}/(g - 1)}{\sum_{\upsilon \in I_{t,i,\textit{u}}}{d_{t, i}(\upsilon, \textit{u})}/I_{t, i, \textit{u}}}$$

where $I_{t,i,\textit{u}}$ is the influence domain of $\textit{u}$ for the time interval [t, i]. We will call this the temporal proximity rank, or TP for short. The temporal proximity rank can be normalized by dividing by i.

Kim and Anderson [2012] proposed temporal closeness, a similar metric which considers all time intervals $[t, i] : t \in [0, i - 1]$.

\begin{equation}
\label{eq:tc}
C_{i,j} = \sum_{i \leq t < j}{\sum_{\textit{u} \in \textit{V}}{\frac{1}{d_{j, t}(\upsilon, \textit{u})}}} \tag{3.}
\end{equation}

When $\textit{u}$ is unreachable from $\upsilon$ over [t, j], $d_{t,j}(\upsilon, \textit{u}) = \infty$. We cover cases where the denominator is infinite by assuming that $\frac{1}{\infty} = 0$. Note that as we are considering a directional graph, $d_{t,j}(\upsilon, \textit{u})$ is not equivalent to $d_{t,j}(\textit{u}, \upsilon)$. To turn Equation (\ref{eq:tc}) into a rank measure, we reverse the direction of the paths to get:

$$C_{i,j}^{P} = \sum_{i \leq t < j}{\sum_{\textit{u} \in \textit{V}}{\frac{1}{d_{t,j}(\textit{u}, \upsilon)}}}$$
We will call this the temporal closeness rank, or TC for notational convenience. The temporal closeness rank can be normalized by dividing by $(|\text{V}| - 1)(j - i)$. 


## Multiplicative Temporal Closeness Rank

When each edge is associated with a probability of transmission, as may be the case in epidemiology models, the probability of a path may be of greater interest than the temporal length. In this case, we can generalize existing methods by considering a digraph where the edge weights are the natural log of the probability. Figure 5 shows a graph of this kind.

```{r, engine = "tikz", echo = FALSE, fig.cap = "A simple temporal network represented as a digraph"}
\begin{tikzpicture}

\tikzset{vertex/.style = {shape=circle,draw,minimum size=1.5em}}
\tikzset{edge/.style = {->,> = latex}}

\node[vertex] (a1) at (0,-1) {$a_{1}$};
\node[vertex] (b1) at (0,1) {$b_{1}$};
\node[vertex] (c1) at (0,3) {$c_{1}$};

\draw[dashed] (1,-2)--(1,4);

\node[vertex] (a2) at (2,-1) {$a_{2}$};
\node[vertex] (b2) at (2, 1) {$b_{2}$};
\node[vertex] (c2) at (2, 3) {$c_{2}$};

\draw[dashed] (3,-2)--(3,4);

\node[vertex] (a3) at (4,-1) {$a_{3}$};
\node[vertex] (b3) at (4,1) {$b_{3}$};
\node[vertex] (c3) at (4,3) {$c_{3}$};

\draw[->, thick] (a1) -> node[near start, below] {0} (a2);
\draw[->, thick] (a2) -> node[near start, below] {0} (a3);
\draw[->, thick] (b1) -> node[near start, below] {0} (b2);
\draw[->, thick] (b2) -> node[near start, below] {0} (b3);
\draw[->, thick] (c1) -> node[near start, below] {0} (c2);
\draw[->, thick] (c2) -> node[near start, below] {0} (c3);
\draw[->, thick] (a1) -> node[near start, below] {-3} (b2);
\draw[->, thick] (c2) -> node[near start, below] {-2} (a3);

\node at (0,-2) {t = 1};
\node at (2,-2) {t = 2};
\node at (4,-2) {t = 3};

\end{tikzpicture}
```

The probability of transmission from an individual to them self is assumed to be one, and hence the natural log becomes 0. Consider a path, P, starting at $\upsilon_{i}$ and ending at $\textit{u}_{j}$. The probability of this path is equal to $\prod_{k = i}^{j}{E_{k}}$, where E is the list of transmission probabilities of path P. The probability of path P can be calculated by:

$$e^{\sum_{w \in E}{log(w)}}$$
All highest-probability paths to $\textit{u}_{i}$ can be calculated with $O(i|V|^{2})$ runtime complexity by a modified version of the Reversed Evolution Network (REN) algorithm proposed by @hanke2017clone. Algorithm 2 shows the psuedocode for this algorithm.


\begin{algorithm}
\RestyleAlgo{boxruled}
\KwResult{Multiplicative Closeness Rank}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{Data file with each row representing a contact and a probability of transmission}
\Output{Multiplicative Closeness Rank for a given node}
\BlankLine
contacts <- List of contact times sorted in increasing order. Each contact time is a data structure with a map of nodes to out-neighbours.\;
\While{Data file has next line}{
  Read line\;
  Add line to contacts using bisection search\;
}
tcp <- Multiplicative closeness rank\;
reachable <- Set of reachable nodes\;
Add target node to reachable\;
sums <- Map of node id to log of the highest probability path (obtained by summing edge weights)\;
back <- Pointer to final contact time in contacts list\;
\While{back is not null}{
  temp <- Map of updated path lengths for this iteration\;
  \ForEach{node in reachable}{
      out-neighbours <- back.out-neighbours[node]\;
      \ForEach{neighbour in out-neighbours}{
          Add neighbour to reachable set\;
          weight <- Edge weight of connection between node and neighbour\;
          temp[neighbour] = Max(temp[neighbour], sums[node] + weight)\;
      }
  }
  \ForEach{node in reachable}{
    sums[node] = Max(temp[node], sums[node])\;
    mcr += $exp{sums[node]}$\;
  }
  Decrement back pointer\;
}
Return tcp\;
\caption{Modified version of the REN algorithm.}
\end{algorithm}

## Absorption Rank

@rocha2014random proposed TempoRank, an extension of the illustrious Google PageRank algorithm to temporal networks. TempoRank considers the stationary distribution of a random walk through the temporal network. However, TempoRank does not generalize well to epidemiology modelling, where infection may be a permanent state. In this section, I propose an aggregate metric for temporal networks. Without loss of generality, we will consider a temporal network consisting of a set of nodes, N, and positive integer contact times, $t = (1, 2, 3, \cdots, T)$. Let $p_{ijk}(t)$ denote the probability of transmission for the k'th contact between individual i and individual j, at time t. Let $n_{ij}(t)$ denote the number of contacts between i and j at time t. Define the i'th row the transition probability matrix
at time t, $\mathbf{B}(t)$, as:

\begin{equation}
\label{eq:absorb}
\mathbf{B}_{ij}(t) = \left\{\begin{matrix}
  1 & i = j, s_{i}(t) = 0 \\
  0 & i \neq j, s_{ij}(t) = 0 \\
  \prod_{m \in N, m \neq i}{\prod_{k = 1}^{n_{im}(t)}{(1 - p_{imk}(t))}} & i = j, s_{i}(t) > 0 \\
  (1 - \mathbf{B}_{ii}(t))(s_{ij}(t)/s_{i}(t)) & i \neq j, s_{ij}(t) > 0  \\
\end{matrix}
\right.
\end{equation}

where $s_{ij}(t)$ and $s_{i}(t)$ are defined as:

\begin{equation}
\label{eq:si}
s_{ij}(t) = 1 - \prod_{k = 1}^{n_{ij}(t)}{(1 - p_{ijk}(t))}
\end{equation}

\begin{equation}
\label{eq:s}
s_{i}(t) = \sum_{j \in N, j \neq i}{s_{ij}(t)}
\end{equation}

$s_{ij}(t)$ represents the probability that at least one contact between i and j at time t is contagious. Now consider the sum:

$$\sum_{j = 1}^{N}{B_{ij}(t)} = B_{ii}(t) + (1 - B_{ii}(t))\sum_{j = 1, j \neq i}^{N}{\frac{s_{ij}(t)}{s_{i}(t)}} = 1$$
Therefore $\mathbf{B}(t)$ is a transition probability matrix. Denote by $\mathbf{B}^{i}(t)$ the transition matrix obtained by setting all off-diagonal elements of the i'th row of $\mathbf{B}(t)$ to zero, and the diagonal element to one. The walk $\mathbf{B}^{i} = (\mathbf{B}^{i}(1), \mathbf{B}^{i}(2), \cdots, \mathbf{B}_{i}(T))$ is an absorbing random walk, and the product $C_{i}^{A}(t) = \pi\mathbf{B}_{i}(1)\mathbf{B}_{i}(2)\cdots\mathbf{B}_{i}(t)$ will be called the absorption rank for individual i at time t, where $\pi$ is a prior probability distribution. For simplicity, we choose $\pi = (\frac{1}{N}, \frac{1}{N}, \frac{1}{N}, ...)$ for all subsequent calculations. The absorption rank of individual i can be (roughly) interpreted as the probability that a random walk passes through i.

If we assume a constant transmission probability, p, for all contacts, \ref{eq:absorb} reduces to:

\begin{equation}
\label{eq:absorb_const}
\mathbf{B}_{ij}(t) = \left\{\begin{matrix}
  1 & i = j, s_{i}(t) = 0 \\
  0 & i \neq j, s_{ij}(t) = 0 \\
  \prod_{m \in N, m \neq i}{(1 - p)^{n_{ij}(t)}} & i = j, s_{i}(t) > 0 \\
  (1 - \mathbf{B}_{ii}(t))(s_{ij}(t)/s_{i}(t)) & i \neq j, s_{ij}(t) > 0  \\
\end{matrix}
\right.
\end{equation}

where $s_{ij}(t)$ is defined as:

\begin{equation}
\label{eq:si_adj}
s_{ij}(t) = 1 - (1 - p)^{n_{ij}(t)}
\end{equation}

and $s_{i}(t)$ is similarly defined as:

\begin{equation}
\label{eq:s_adj}
s_{i}(t) = \sum_{j \in N, j \neq i}{s_{ij}(t)}
\end{equation}

In practice, p may be estimable from domain-specific knowledge. A value of $p = 0.3$ was chosen for all subsequent calculations.

<!-- ## Example -->

<!-- A simple example network of five nodes is shown in Example_Network.csv. For simplicity, a constant transmission probability was used for all contact events. In total, 500 simulations were run on this network. The absorption centrality and the number of times infected were calculated for the final contact time. Table 1 shows the correlation between the absorption centrality and the probability of infection by the end of the simulation. -->


<!-- # ```{r, table.cap = "Pearson Correlation between temporal absorption centrality and proportion of times infected over 500 simulations", echo = FALSE} -->
<!-- # corr = c() -->
<!-- #  -->
<!-- # df = read.csv("Results1.csv") -->
<!-- # df = df[, 1:5] -->
<!-- #  -->
<!-- # colnames(df) = c("Node1", "Node2", "Node3", "Node4", "Node5") -->
<!-- #  -->
<!-- # prop = c() -->
<!-- #  -->
<!-- # for (col in colnames(df)) { -->
<!-- #   prop = c(prop, sum(df[,col] <= 254) / 500) -->
<!-- # } -->
<!-- #  -->
<!-- # centrality = read.csv("AbsorptionCentrality1.csv") -->
<!-- #  -->
<!-- # colnames(centrality) = c("Node", "Centrality") -->
<!-- #  -->
<!-- # res = rev(centrality$Centrality) -->
<!-- #  -->
<!-- # corr = c(corr, cor(x = prop, y = res, method = "pearson")) -->
<!-- # ``` -->
<!-- #  -->
<!-- # ```{r, table.cap = "Pearson Correlation between temporal absorption centrality and number of times infected over 500 simulations", echo = FALSE} -->
<!-- #  -->
<!-- # df = read.csv("Results2.csv") -->
<!-- # df = df[, 1:5] -->
<!-- #  -->
<!-- # colnames(df) = c("Node1", "Node2", "Node3", "Node4", "Node5") -->
<!-- #  -->
<!-- # prop = c() -->
<!-- #  -->
<!-- # for (col in colnames(df)) { -->
<!-- #   prop = c(prop, sum(df[,col] <= 254) / 500) -->
<!-- # } -->
<!-- #  -->
<!-- # centrality = read.csv("AbsorptionCentrality2.csv") -->
<!-- #  -->
<!-- # colnames(centrality) = c("Node", "Centrality") -->
<!-- #  -->
<!-- # res = rev(centrality$Centrality) -->
<!-- #  -->
<!-- # corr = c(corr, cor(x = prop, y = res, method = "pearson")) -->
<!-- #  -->
<!-- # data = data.frame(Correlation = corr) -->
<!-- #  -->
<!-- # rownames(data) = c("p = 0.2", "p = 0.1") -->
<!-- # ``` -->
<!-- #  -->
<!-- # ```{r, echo = FALSE} -->
<!-- # knitr::kable(data, caption = "Pearson correlation between temporal absorption centrality and infected proportion over 500 simulations") -->
<!-- # ``` -->

## Simulation

Epidemic simulations typically follow the standard Markovian framework, in which we assume transmission depends only on the current state of the network. The temporal network is represented by a list of snapshots of the network sorted by time, and it is assumed that during each snapshot, only one "hop" can occur. In other words, if we have two contacts (i, k, t) and (k, j, t), i cannot infect j (via k) at time t. The standard Markovian framework typically employs simplifying assumptions. The Susceptible-Infected-Recovered (SIR) model is a common implementation which assumes that all individuals are susceptible at the beginning, and once infected they remain infectious for a recovery period. Other common implementations include the Susceptible-Infected (SI) model and the Susceptible-Infected-Susceptible (SIS) model. We use the Susceptible-Infected (SI) framework for all simulations, in which infection is a permanent state.

### Epidemic Simulations for Static Networks

This section describes a novel approach to epidemic modelling using static networks. A social network is modeled by a static graph, where each node represents an individual, and each edge represents an ongoing, time-independent relationship between two individuals. It is assumed that contacts between any two individuals i and j follow a Poisson Process with a rate parameter of $\lambda_{ij}$, and each contact has a constant probability, $\beta$, of being infectious. The sojourn times of contagious contacts between i and j follow an exponential distribution with a rate of $\beta\lambda$. Thus, the sojourn times of infectious contacts between i and j follow independent exponential distributions with a fixed rate parameter of $\beta\lambda_{ij}$. Due to the memorylessness property of the exponential distribution, the time until the next infectious contact, from any starting time, follows the same distribution. By these assumptions, epidemics can be efficiently simulated in the most general case, without prior knowledge of contacts. Consider a simulation ending at time T, which starts with a set of infected nodes. We iterate through all neighbours of each infected node, and sample a time until the next infectious contact, adding it to a sorted list of infection times as we go. At each subsequent step, the smallest infection time is selected from the list. If this time is greater than T, the simulation stops. Otherwise we sample an infection time for each neighbour of the infected node. If the sampled infection time for a given node is greater than it's current infection time, it is ignored. Figure 2 shows a static graph representing a small social network, where the edge weights correspond to the rate parameter, $\beta\lambda_{ij}$. Consider a simulation on this graph which terminates at the time T = 100. Initially, only A is infected. We sample t = 60, 40, 50 for neighbours C, B and D respectively. In the next step, the infection time of C (t = 40) is removed from the list, and the time 70 is sampled for node E. Thus, the infection time for E is 40 + 70 = 110. Then, the infection time of node D (t = 50) is removed from the list, and each neighbour is considered in turn. Suppose we sample a sojourn time of 70 for E, hence the new infection time is 50 + 70 = 120 which is greater than 110, thus it is ignored. Likewise, suppose the time 40 is sampled for the infection process between D and B. The new infection time for B is 50 + 40 = 90 which is greater than 60, thus it is ignored.

```{r, engine = "tikz", echo = FALSE, fig.cap = "A simple social network represented as a static graph"}
\begin{tikzpicture}

\tikzset{vertex/.style = {shape=circle,draw,minimum size=1.5em}}
\tikzset{edge/.style = {->,> = latex}}

\node[vertex, label = below:{t = 0}] (a1) at (0,-1) {$A$};
\node[vertex, label = right:{t = 60}] (b1) at (1,3) {$B$};
\node[vertex, label = right:{t = 40}] (c1) at (3,3) {$C$};
\node[vertex, label = left:{t = 50}] (d1) at (-1,5) {$D$};
\node[vertex, label = right:{t = 110}] (e1) at (3,7) {$E$};

\draw[thick] (a1) -- node[near start, right] {2} (b1);
\draw[thick] (a1) -- node[near start, right] {3} (c1);
\draw[thick] (a1) -- node[near start, right] {4} (d1);
\draw[thick] (e1) -- node[near start, right] {2} (c1);
\draw[thick] (d1) -- node[near start, below] {1} (e1);
\draw[thick] (d1) -- node[near start, below] {3} (b1);

\end{tikzpicture}
```

In general, contact rate parameters may be estimable from contact tracing data or theoretical values. It should be clear that this method is not a replacement for traditional simulation approaches, and it should only be used when underlying assumptions (contacts follow a poisson process and a constant transmission probability) are verified.

### Algorithms

Simulations were carried out using the event-based algorithm proposed by @kiss2017mathematics, and described in detail by @holme2021fast. To understand this algorithm, we will first consider a naive approach:

\large Naive Algorithm \normalsize

1. Initialize all starting nodes as infectious.

2. Initialize all non-starting nodes as susceptible.

3. Go through the contacts in increasing order of time.

4. Whenever a node gets infected, change it's state to infectious.

5. Stop the simulation when no more nodes can be infected.

Now consider two nodes, i and j, and suppose that i is infected at $t = 0$. There could be thousands of contacts between i and j, however only one of these contacts is contagious.  Clearly we can avoid many unnecessary iterations if we find the first infectious contact in a single step. Now suppose that all contacts are assumed to have a constant transmission probability of $\beta$. Then, the index of the first infectious contact between i and j follows a geometric distribution. The probability that the k'th contact is infectious is given by

$$\beta(1 - \beta)^{k - 1}$$
One can sample k by

\begin{equation}
\label{eq:sample}
\lceil \frac{log(1 - X)}{log(1 - \beta)} \rceil
\end{equation}

where $\beta$ is the fixed transmission probability and $X \sim Uniform(0, 1)$.

The event-based algorithm relies on several user-defined data structures:

Node: A data structure with a unique id and an infection time (**t_inf**) attribute. In addition, each node contains a list of neighbours and a list of contact times (sorted in increasing order) for each neighbor, as well as a list of associated transmission probabilities if necessary. The contact times must be sorted in increasing order to enable a bisection search (the significance of this will become clear).

Heap: A min-heap data structure for storing node id's of infected nodes, ordered by earliest infection time, with $\text{O}(\log(\text{n}))$ "*up-heap*" and "*down-heap*" operations, where $n$ is the number of elements in the heap. A min-heap is conceptually a tree where the root node is the lowest order element for any subtree (including the entire tree), which in this case is the earliest unprocessed infection event. The exact implementation of the heap is unimportant, however the min-heap property must be restored, whenever a node is added or removed, via the *up-heap* and *down-heap* operations.

At the beginning of the simulation, **t_inf** is set to T (the end time of the simulation) for all nodes. All start nodes are added to the heap and their infection times (**t_inf**) are set to 0. At each step, the earliest infection event is removed from the heap and processed. When an infected node is processed, it's neighbours are considered in turn. Suppose node i is infected at time $\text{t\_inf}_{i} = 20$, and node j is a neighbour of node i. If $\text{t\_inf}_{j} < 20$ then node j is skipped. Otherwise, a bisection search (an efficient search algorithm for sorted lists) is carried out to find the earliest contact time, $t$, such that $t \geq 20$. Suppose there are $m$ possible infectious contacts between nodes i and j, and we sample an integer, k, by Equation (\ref{eq:sample}). If $k \geq m$ then we continue, otherwise we add node j to the heap and update $\text{t\_inf}_{j}$. If j is infected a second time at time t, two possibilities must be considered:

1. If $\text{t\_inf}_{j} \leq t$, then j is already infected and should not be added to the heap twice.

2. If $\text{t\_inf}_{j} > t$, then $\text{t\_inf}_{j}$ is set to $t$ and the position of j in the heap must be updated to restore the min-heap property.

This process is repeated until no more nodes can be infected. The runtime of the event-based algorithm is proportional to the number of nodes in the network, whereas the runtime of the naive algorithm is proportional to the number of contact events. The event-based algorithm is recommended for dense networks with many contacts, where computational efficiency is required.

All simulations were carried out by the event-based algorithm, and all contacts were assumed to be symmetric, so each snapshot is an undirected network.


# Results

## Validation of Adjusted Percolation Centrality

Previously, we discussed how epidemics could be simulated by static networks. We use this approach to compare percolation centality and adjusted percolation centrality. The importance of a given node was tested empirically by calculating the difference in average reproductive number (over 100 simulations) between graphs which include and exclude the node in question. A high difference in reproductive numbers indicates that the node is instrumental in the spread of the disease.

The Erdos-Renyi model (@erdos1960evolution) was used to generate random graphs of 40 nodes (four of which are initially infected), and the connectedness property (an underlying assumption of betweenness centrality) was verified for each graph. We start with an edge between every vertex, and each edge is included with a constant probability, p. Figure 7 shows the results for various edge inclusion probabilities. Both variants of percolation centrality showed similar correlations with the empirical measure of centrality, which indicates that our modified version of percolation centrality does not perform better.

```{r, echo = FALSE, fig.cap = "Correlation between the difference of reproductive numbers and the two variants of percolation centrality for different edge inclusion probabilities"}
setwd("C:\\Users\\nickw\\Downloads\\SI_Model")
knitr::include_graphics("AdjCentrality.png", rel_path = TRUE)
```

This simulation approach may be used to compare centrality metrics, and choose an appropriate metric for a given problem. Although we demonstrate how this simulation approach can be used, we turn our attention to approaches that are less restrictive in their assumptions. 


## Comparison of Temporal Centrality Metrics

Six centrality measures were calculated for the CovidCard dataset, namely Temporal Closeness (denoted by TC) and Proximity Rank (denoted by TP), Absorption Rank, Temporal Katz Centrality, Temporal Degree Centrality and Temporal Betweenness Centrality. The metrics were chosen because they satisfy two key assumptions for temporal disease-transmission network: Snapshots are not required to be strongly connected and only one "step" may occur for each snapshot. For a fair comparison, all centrality measures were calculated by assuming constant transmission probabilities for every contact event. This was done because Temporal Katz Centrality does not generalize to varying transmission probabilities. These measures were correlated to the observed number of times each individual was infected over 751,000 simulations (shown in Figure 8). The dataset contained 751 individuals in total, and 1000 simulations were carried out for each of the 751 possible starting nodes, adding up to 751,000 simulations in total.

```{r, results = 'hide', echo = FALSE, fig.cap = "Spearman rank correlations between centrality measures and observed likelihood of infection."}
suppressPackageStartupMessages(library(dplyr))
library(ggplot2)

katz <- read.csv("Katz_Centrality.csv")
katz <- katz %>% mutate(Centrality = round(Centrality, 6), TempDegree = round(TempDegree, 6))

tpp <- read.csv("tpp.csv", header = FALSE)

absorb <- read.csv("AbsorptionCentrality.csv")
absorb <- absorb[rev(rownames(absorb)), ]

colnames(absorb) <- c("Node", "Absorb")
colnames(tpp) <- c("Node", "TC", "TP")

katz$Node = round(katz$Node, 0)


tbc_data <- read.csv("tbc.csv")

tbc <- round(tbc_data$tbc, 9)

results = read.csv("Results.csv")
results = results[1:751, 1:751]

results[] = lapply(results, function(x) ifelse(x > 1000, 0, x))

probs = apply(results, 2, mean)/1000

corKatz = cor(probs, katz$Centrality, method = "spearman")
corDeg = cor(probs, katz$TempDegree, method = "spearman")
corAbs = cor(probs, absorb$Absorb, method = "spearman")
corTp = cor(probs, tpp$TC, method = "spearman")
corTpp = cor(probs, tpp$TP, method = "spearman")
corTbc = cor(probs, tbc, method = "spearman")

correlations = c(corKatz, corDeg, corAbs, corTp, corTpp, corTbc)

names <- c("Katz Centrality", "Degree Centrality", "Absorption Centrality", "Temporal Closeness", "Temporal Proximity", "Temporal Betweenness Centrality")

df <- data.frame(type = names, corr = correlations)

df %>% ggplot(aes(x = type, y = corr)) + geom_bar(stat = "identity", color = "blue", fill = "blue") + xlab("Centrality Measure") + ylab("Spearman Rank Correlation") + scale_x_discrete(labels = c("Absorption", "Degree", "Katz", "TBC", "TC", "TP"))
```
```{r, echo = FALSE, fig.cap = "Pearson correlations between centrality measures and observed likelihood of infection."}
corKatz = cor(probs, katz$Centrality)
corDeg = cor(probs, katz$TempDegree)
corAbs = cor(probs, absorb$Absorb)
corTp = cor(probs, tpp$TC)
corTpp = cor(probs, tpp$TP)
corTbc = cor(probs, tbc)

correlations = c(corKatz, corDeg, corAbs, corTp, corTpp, corTbc)

df <- data.frame(type = names, corr = correlations)

df %>% ggplot(aes(x = names, y = corr)) + geom_bar(stat = "identity", color = "blue", fill = "blue") + xlab("Centrality Measure") + ylab("Pearson Correlation") + scale_x_discrete(labels = c("Absorption", "Degree", "Katz", "TBC", "TC", "TP"))
```

The Spearman Correlations are generally large, with Absorption Rank showing a larger correlation ($\rho \approx 0.8$) compared to other metrics. Conversely, the metrics generally showed lower Pearson correlations, especially Katz centrality. This suggests that the metrics should not be used for prediction, rather they should be used for ranking the relative importance of individuals within the same network. TC and TP performed similarly in terms of ranking individuals, however TP outperformed TC in terms of Pearson Correlation.

```{r, echo = FALSE, fig.cap = "Correlations between the five centrality measures."}
library(ggcorrplot)

data <- data.frame(TC = tpp$TC, TP = tpp$TP, Absorb = absorb$Absorb, Katz = katz$Centrality, Degree = katz$TempDegree, TBC = tbc)

corr <- cor(data)

ggcorrplot(corr, lab = TRUE)
```

Figure 8 shows the Pearson correlations between the six metrics. Except for Temporal Closeness Rank and Temporal Proximity Rank, the metrics are mostly independent, with many weak or negative correlations. This suggests that the metrics are tapping into different effects, and our rankings could be improved by combining many metrics as predictors in a regression model.

## Model Selection

```{r, echo = FALSE, fig.cap = "Standardized residuals of the model plotted against theoretical quantiles of the normal distribution."}
data$likelihood = probs

model = lm(likelihood ~ TC + TP + Absorb + Katz + Degree + TBC, data = data)

residuals <- residuals(model)

qqnorm(residuals)
qqline(residuals)
```

Figure 9 shows the Q-Q plot for a linear regression model predicting the observed likelihood of infection, with the five centrality metrics as predictors. The relationship is clearly non-linear, with a large number of extreme values for the residuals. A transformation of the output variable may satisfy the linearity assumption, especially if the distribution is skewed.

```{r, echo = FALSE, fig.cap = "Pairwise relationships between the centrality metrics and the output variable (likelihood)", message = FALSE}
library(GGally)

ggpairs(data)
```

Looking at the pairwise relationships (shown in Figure 10), the assumptions of linear regression clearly cannot be satisfied. The output variable (plotted in the bottom-right corner) is bimodal and not amenable to common transformations (e.g. log, square root, polynomial). Machine Learning and Artifical Intelligence are often used for regression when the distribution of the data is non-standard. We use three machine learning methods - Gradient Boosting Regression, Decision Trees and Polynomial Regression - to predict the likelihood of infection from the six centrality metrics. 

Polynomial Regression is based on a similar concept to linear regression, however it allows for powers greater than one in the regression equation (e.g. $y = \beta_{1}X + \beta_{2}X^{2}$). Closed-form solutions are rarely known, so the coefficients must be "learned" by numerical optimization methods. Decision Trees split the data into progressively smaller subgroups using decision rules (e.g. X > 40), and each group (sometimes called a leaf node) is assigned a predicted value. The prediction for a given observation is then the predicted value of it's respective leaf node, which is determined by the decision rules. At each branch of the tree, the optimal decision rule is found by minimizing a splitting criteria (e.g. Sum of Squared Errors (SSE), Mean Squared Error (MSE)). Gradient Boosting Regression takes a weighted average of predictions for many decision trees. This may lead to better predictions, especially for high-dimensional or noisy data.

The data was split into training and test sets, comprising 75% and 25% of the data respectively. Hyper parameters were optimized by cross-validation on the training set and evaluated on the test set. For each model, the predictive power (correlation between predictions and observations) was calculated by Pearson and Spearman correlations.

```{r, echo = FALSE, fig.cap = "Predicted vs Actual values after training Gradient Boosting model and applying to test set.", cache = TRUE, warning = FALSE, message = FALSE}
library(rsample)
library(Matrix)
suppressPackageStartupMessages(library(xgboost))

set.seed(1374)

data_split <- initial_split(data, prop = 0.75)

train <- training(data_split)

train[] = lapply(train, as.numeric)

xTrain = train[, -which(names(train) %in% c("likelihood"))]

# pca <- prcomp(xTrain, scale. = TRUE)

# rotation <- pca$rotation

# xTrain = as.matrix(xTrain) %*% rotation

# xTrain <- xgb.DMatrix(data = matrix(train[, -which(names(data) == "likelihood")]))
# yTrain <- xgb.DMatrix(data = matrix(train$likelihood))

test <- testing(data_split)

xTest <- test[, -which(names(test) %in% c("likelihood"))]
names(xTest) <- names(xTrain)

# xTest = as.matrix(xTest) %*% rotation

yTest <- test$likelihood

params = list(objective = "reg:absoluteerror", eval_metric = "rmse")

dtrain = xgb.DMatrix(data = data.matrix(xTrain), label = data.matrix(train$likelihood))

cv_results <- xgb.cv(params = params, data = dtrain, nfold = 10, nrounds = 500, early_stopping_rounds = 150, verbose = FALSE)

best_round = cv_results$best_iteration

model = xgb.train(params = params, data = dtrain, nrounds = best_round)

y_pred = predict(model, newdata = xgb.DMatrix(data.matrix(xTest)))

corXG <- cor(y_pred, yTest, method = "spearman")

SST <- sum((yTest - mean(yTest))^{2})/length(yTest)

rXG <- cor(y_pred, yTest, method = "pearson")

df <- data.frame(predicted = y_pred, actual = yTest)

## df %>% ggplot(aes(x = predicted, y = actual)) + geom_point() + geom_smooth(color = "blue")
```

```{r, echo = FALSE}
library(rsample)
library(rpart)
library(rpart.plot)
suppressPackageStartupMessages(library(Metrics))

decTree <- rpart(likelihood ~ ., data = train)
```

```{r, echo = FALSE}
SST <- sum((yTest - mean(yTest))^{2})/length(yTest)

y_pred = predict(decTree, newdata = xTest)

corTree = cor(y_pred, yTest, method = "spearman")

rTree <- cor(y_pred, yTest, method = "pearson")

## plot(y_pred, test$likelihood, xlab = "Predicted Values", ylab = "Observed Values", main = "Predicting likelihood of infection with Decision Tree.")
```

```{r, echo = FALSE, cache = TRUE}
library(mltools)

x <- data[, -which(names(data) == "likelihood")]

y <- data$likelihood
degree <- 7

polynomial_optimizer <- function(par) {
  prediction <- numeric(nrow(data))
  for (i in 1:length(par)){
    col = as.vector(x[, ((i - 1) %/% degree) + 1])
    power = (i %% degree) + 1
    
    prediction = prediction + col * (par[i]^power)
  }
  
  rmse(prediction, y)
}

polynomial_predictor <- function(params) {
  
  prediction <- numeric(nrow(data))
  for (i in 1:length(params)){
    col = as.vector(data[, ((i - 1) %/% degree) + 1])
    power = (i %% degree) + 1
    
    prediction = prediction + col * (params[i]^power)
  }
  
  prediction
  
}

parameters = rep(0.1, times = 15)

model <- optim(par = parameters, fn = polynomial_optimizer, method = "L-BFGS-B", control = list(maxit = 1000))

coefs = model$par

y_pred = polynomial_predictor(coefs)

SST <- sum((y - mean(y))^{2})/length(y)

corPoly <- cor(y_pred, y, method = "spearman")
rPoly <- cor(y_pred, y, method = "pearson")

## plot(y_pred, data$likelihood, xlab = "Predicted Values", ylab = "Observed Values", main = "Predicting likelihood of infection with Polynomial Regression")
```

```{r, echo = FALSE, fig.cap = "Spearman rank correlations between predictions and observations for three machine learning algorithms."}
correlations = c(corPoly, corTree, corXG)

names = c("Polynomial Regression", "Decision Tree Regression", "XGBoost Regression")

df <- data.frame(type = names, corr = correlations)

df %>% ggplot(aes(x = names, y = corr)) + geom_bar(stat = "identity", color = "blue", fill = "blue") + xlab("Centrality Measure") + ylab("Predictive Power (Spearman Correlation)")
```

```{r, echo = FALSE, fig.cap = "Pearson correlations between predictions and observations for three machine learning algorithms.", output = FALSE}

df$corr <- c(rPoly, rTree, rXG)

df %>% ggplot(aes(x = names, y = corr)) + geom_bar(stat = "identity", color = "blue", fill = "blue") + xlab("Centrality Measure") + ylab("Predictive Power (Pearson Correlation)")
```

Figures 11 and 12 show the predictive power. All algorithms performed worse than Absortion Rank on both metrics. XGBoost and polynomial regression both performed poorly, whereas the decision tree performed similarly to Absorption Rank, therefore we subsequently use decision trees for all further modelling.


## Feature Selection

If some predictors in a model are unimportant, we risk losing model performance due to the "Curse of Dimensionality". Many unimportant predictors may result in increased over fitting and difficulty with finding meaningful patterns in the data. Feature selection methods are used to identify and remove unimportant predictors in order to increase simplicity and performance of models. The feature importance scores were calculated for a decision tree with all six predictors.

```{r, echo = FALSE, fig.cap = "Feature importance scores of the six features in the decision tree."}
scores <- decTree$variable.importance

barplot(scores, col = "blue", main = "Variable Importance")
```

Katz centrality, Degree Centrality and Absorption Centrality all showed high variable importance scores. However, highly correlated predictors (for instance, TP and TPP) may have attenuated importance rankings due to multicollinearity. For this reason, iterative feature selection methods (testing many combinations of features systematically) are often used in lieu of feature importance scores.

We test all possible combinations of two or more features by 5-fold cross validation using decision tree regression. Each model was tested with four different values of the complexity parameter (0.0001, 0.001, 0.006, 0.01), a regularization criterion which limits the complexity of the tree in order to prevent over-fitting. Models were evaluated by the Mean Squared Error (MSE) metric with an estimated standard error, and the five best-performing models are shown in Table 1.

```{r, echo = FALSE, message = FALSE}
library(caret)

split <- initial_split(data, prop = 0.75)

train <- training(split)

test <- testing(split)

xTest <- test[, -which(names(test) %in% c("TP", "TBC", "likelihood"))]

yTest <- test$likelihood

train <- train[, -which(names(train) %in% c("TP", "TBC"))]

tree <- rpart(likelihood ~ ., data = train)
```

```{r, echo = FALSE, cache = TRUE}
# Load the "doParallel" package for parallel processing.
library(doParallel)
# Load the "foreach" package for splitting loops across multiple cores.
library(foreach)

suppressPackageStartupMessages(library(pander))

registerDoParallel(cores = 4)

all.comb <- expand.grid(var1 = c(0, 1), var2 = c(0, 1), var3 = c(0, 1), var4 = c(0, 1), var5 = c(0, 1), var6 = c(0, 1))[-1, ]
```

```{r, echo = FALSE, cache = TRUE, warning = FALSE}
set.seed(485434)

all.comb <- all.comb[rowSums(all.comb) > 1,]

var.indices <- which(!(names(data) == "likelihood"))

data.shuffled <- data[sample(1:nrow(data), nrow(data)), ]

data.shuffled <- as.data.frame(lapply(data.shuffled, as.numeric))

names(data.shuffled) <- names(data)

sample_size <- floor(0.85 * nrow(data.shuffled))

indices <- sample(1:nrow(data.shuffled), size = sample_size, replace = F)

holdout <- data.shuffled[-indices, ]

names(holdout) <- names(data.shuffled)

data.shuffled <- data.shuffled[indices, ]

folds <- 5

MSE.extract <- function(x) {
  return(as.numeric(min(x$results$RMSE)) ^ 2)
}

cp_vals <- data.frame(cp = c(0.0001, 0.001, 0.006, 0.01))

fitControl <- trainControl(method = "cv", number = folds)

all.models.fit.cv <- foreach(i = 1 : nrow(all.comb), .packages = "caret") %dopar%
  {
    model.equation <- as.formula(paste("likelihood ~ ", paste(names(data)[var.indices][all.comb[i, ] == 1], collapse = " + ")))
    train(model.equation, data = data.shuffled, method = "rpart", trControl = fitControl, metric = "RMSE", tuneGrid = cp_vals)
  }

MSE.rep.cv <- sapply(all.models.fit.cv, MSE.extract)

MSE.ordered <- order(MSE.rep.cv)

# Construct a matrix in which to store information on which variables are included in the 10 best models.
best.models <- matrix(NA, nrow = 5, ncol = length(var.indices), dimnames = list(NULL, names(data)[var.indices]))
```

```{r, echo = FALSE}
suppressPackageStartupMessages(library(Metrics))

calcSE <- function(model) {
  MSE <- model$resample$RMSE^{2}
  return(sd(MSE))
}

for (i in 1:5)
{
  for (j in 1:ncol(best.models)) {
    
    if (all.comb[MSE.ordered[i], j] == 1) {
      best.models[i, j] = names(data[var.indices])[j]
    }
    
    else {
      best.models[i, j] = "N/A"
    }
    
  }
}

fun <- function(row) {
  ifelse(row == "N/A", "\u2717", "\u2713")
}

new_names <- c("MSE", "Standard Error", colnames(best.models))

best.models <- apply(best.models, 1, fun)

best.models <- matrix(best.models, nrow = 5)

mseSD <- sapply(all.models.fit.cv[MSE.ordered[1:5]], calcSE)

best.models <- cbind(round(as.numeric(unlist(MSE.rep.cv[MSE.ordered[1:5]])), 6), round(mseSD, 6), best.models)

colnames(best.models) <- new_names

pander::pander(best.models, caption = "Five best performing feature combinations in cross-validation and their MSE")
```

```{r, echo = FALSE}
stanDev <- sd(MSE.rep.cv)

nums <- 1:length(MSE.rep.cv)

indices <- nums[MSE.rep.cv <= MSE.rep.cv[MSE.ordered[1]] + stanDev]

holdoutMSE <- rep(NA, 5)

X = holdout[,-c(names(holdout) == "likelihood")]

names(X) <- names(holdout)[var.indices]

Y = holdout$likelihood

i = 1

for (ind in MSE.ordered[1:5]){
  holdoutMSE[i] <- mse(predict(all.models.fit.cv[[ind]], newdata = X[, predictors(all.models.fit.cv[[ind]])]), Y)
  i = i + 1
}

best.models <- matrix(NA, nrow = 5, ncol = length(var.indices), dimnames = list(NULL, names(data[var.indices])))

for (i in 1:5)
{
  for (j in 1:ncol(best.models)) {
    
    if (all.comb[MSE.ordered[i], j] == 1) {
      best.models[i, j] = names(data[var.indices])[j]
    }
    
    else {
      best.models[i, j] = "N/A"
    }
    
  }
}

fun <- function(row) {
  ifelse(row == "N/A", "\u2717", "\u2713")
}

new_names <- c("MSE", colnames(best.models))

best.models <- apply(best.models, 1, fun)

best.models <- matrix(best.models, nrow = 5)

best.models <- cbind(round(as.numeric(unlist(holdoutMSE)), 6), best.models)

colnames(best.models) <- new_names

pander::pander(best.models, caption = "MSE of five best-performing feature combinations on the holdout set")
```

The best-performing model in the cross-validation phase used Absorption Rank, Temporal Katz Centrality, Temporal Betweenness Centrality and Temporal Closeness Rank. The standard deviation of cross-validation MSE scores was calculated for all feature combinations, as well as the MSE on the holdout set (shown in Table 2). All models had lower MSE estimates during cross-validation, which potentially indicates over-fitting. MSE estimates are typically decreased during cross-validation for several reasons, including information leakage wherein data in the validation set influences hyperparameter choices, leading to bias. It should be clear that the holdout set MSE is not used for ranking models, and it should be interpreted as an unbiased estimate of the Mean Squared Error. 

The data was split into training and test sets comprising 60% and 40% of the data respectively. Each centrality metric was fitted on the training set by a simple linear regression model with one predictor. A decision tree was fitted on the training set by using the best-performing hyperparameters and feature set chosen during cross-validation. Figure 14 compares the proportion of variance ($R^{2}$) explained by each centrality metric and the decision tree when applied to the test set. 

```{r, echo = FALSE, message = FALSE, cache = TRUE, fig.lab = "Predicted values vs Observed values for the decision tree", warnings = FALSE, output = FALSE}
library(ggplot2)

split <- initial_split(data, prop = 0.6)

train <- training(split)

test <- testing(split)

xTest <- test[, -which(names(test) == "likelihood")]

yTest <- test$likelihood

optimal_cp <- all.models.fit.cv[[MSE.ordered[1]]]$bestTune[1]

tree <- rpart(likelihood ~ TP + Absorb + Katz + TBC, data = train, cp = optimal_cp)

predictions <- predict(tree, newdata = xTest)

df <- data.frame(x = predictions, y = yTest)

## df %>% ggplot(aes(x = x, y = y)) + geom_point() + xlab("Predicted Values") + ylab("Observed Values") + ggtitle("Comparison of predictions from best model against observed values")
```

```{r, echo = FALSE, fig.cap = "Comparison of RMSE for final decision tree and individual predictors", warning = FALSE}
suppressPackageStartupMessages(library(Metrics))
suppressPackageStartupMessages(library(modelr))

SST <- sum((yTest - mean(yTest))^{2})/length(yTest)


full_rmse <- 1 - (Metrics::mse(predictions, yTest)/SST)
abs_rmse <- rsquare(lm(likelihood ~ Absorb, data = train), data = test)
katz_rmse <- rsquare(lm(likelihood ~ Katz, data = train), data = test)
deg_rmse <- rsquare(lm(likelihood ~ Degree, data = train), data = test)
tp_rmse <- rsquare(lm(likelihood ~ TC, data = train), data = test)
tpp_rmse <- rsquare(lm(likelihood ~ TP, data = train), data = test)
tbc_rmse <- rsquare(lm(likelihood ~ TBC, data = train), data = test)

rmse_vec <- c(full_rmse, abs_rmse, deg_rmse, katz_rmse, tp_rmse, tpp_rmse, tbc_rmse)

names(rmse_vec) <- c("Tree", "Absorb", "Degree", "TP", "TBC", "TC", "Katz")

barplot(rmse_vec, main = "R squared for the optimized decision tree and six predictors", xlab = "Model", ylab = "R squared", col = "blue")
```

The Decision Tree model showed the highest R-squared value, followed closely by Absorption Rank and Degree Centrality. It should be clear that these rankings are only meaningful in the context of prediction, where the size of prediction error is important. When ranking the relative importance of nodes in a network, other metrics should be given more weight (for instance, Spearman Rank Correlation). These results suggest that researchers should include multiple temporal centrality metrics in statistical models for improved results.

```{r, echo = FALSE, eval = FALSE}

corPear <- cor(predictions, yTest, method = "spearman")
corSpear <- cor(predictions, yTest, method = "pearson")

cors <- c(corPear, corSpear)

names(cors) <- c("Pearson Correlation", "Spearman Correlation")

barplot(cors, col = "blue", main = "Predictive Power for final decision tree")
```


## Constant Transmission Probability

The constant transmission probability assumption is often used to reduce computation time of simulations.

To empirically test the constant transmission probability assumption, the simulated probability of infection for 751 individuals was compared for two different algorithms (run on the CovidCard dataset). In the first algorithm, the probability of transmission is calculated separately for every contact (see the Estimation of Transmission Probability Section for details). In the second algorithm, we assume that for two nodes i and j, the probability of transmission is a constant value $p_{ij}$, regardless of the duration and proximity of the contact event, and this probability is estimated by averaging the event-specific probabilities $p_{ijt}$ (where t is a time index) over all contacts between i and j. As in the first algorithm, event-specific probabilities are calculated by the same logistic regression model.

In total, 1000 simulations were run for each algorithm. For each run, the indicator variable, $I_{i}$, is 1 if node i was infected, and 0 otherwise. The simulated probability of infection for node i is the average of $I_{i}$ over all 1000 runs.

Denote by $\hat{P^{\mathbf{1}}}$ the vector of observed probabilities for algorithm 1 (and likewise for algorithm 2). Thus, $\hat{P_{i}^{\mathbf{1}}} - \hat{P_{i}^{\mathbf{2}}}$ is the observed difference between the two algorithms for node i. Suppose we want to test whether this difference is significant. The test statistic is

$$Z = \frac{\hat{P_{i}^{\mathbf{1}}} - \hat{P_{i}^{\mathbf{2}}}}{\hat{P_{i}}(1 - \hat{P_{i}})(\frac{1}{1000} + \frac{1}{1000})}$$
where $\hat{P_{i}}$ is the pooled proportion under the null hypothesis ($P_{i}^{\mathbf{1}} = P_{i}^{\mathbf{2}}$). The test statistic converges to a standard normal distribution when the true proportions are equal, however the normal approximation fails for proportions which are close to zero or one. Consequently, we turn our attention to alternative approaches with less restrictive distributional assumptions.

Fisher's exact test is an exact test of association for two-way contingency tables. Combinatorial mathematics can be used to give an exact p-value, although the calculations become intractable for high cell counts. Consequently, p-values are often simulated to reduce computation. For each individual, we create a 2-by-2 contingency table from the variables algorithm (constant or varying transmission probabilities) and infected (yes or no). In total, 751 pairwise comparisons (one for each individual) were carried out. Due to the large number of tests, multiple comparisons were controlled for to reduce the risk of Type I errors. Several multiple comparison adjustment methods were considered.

Power and Type I error rate are two important factors which influence our choice of multiple comparison adjustment. Power is defined as the probability of falsely accepting the null hypothesis, whereas the Type I error rate is the probability of falsely rejecting the null hypothesis. Multiple comparison adjustments aim to reduce the family-wise Type I error rate by controlling the significance level of each individual comparison. We discuss several common adjustment methods and their use cases:


### Bonferroni Correction:

The Bonferroni correction, proposed by @dunn1961multiple, is a basic method which makes no assumptions about dependency between tests. For a given significance level $\alpha$, the Bonferroni correction guarantees a family-wise Type I error rate $\leq \alpha$. The test-wise significance level is set to $\frac{\alpha}{N}$, where N is the number of pairwise tests. The Bonferroni correction is simple and robust, though it lacks power compared to other methods.


### Holm's Method:

Holm's method, proposed by @holm1979simple, is a powerful alternative to the Bonferroni correction. Holm's method tests the hypotheses iteratively, updating the significance level at each step. First, the p-values of pairwise tests are sorted in increasing order, and the sequential tests begin from the lowest p-value. The algorithm starts with a significance level of $\frac{\alpha}{N}$. If the first result is non-significant, the second result is tested with a significance level of $\frac{\alpha}{N - 1}$. In general, the i'th test statistic is evaluated with a significance level of $\frac{\alpha}{N - i + 1}$. Holm's method makes no assumptions about dependency between pairwise tests, and guarantees a family-wise Type I error rate less than $\alpha$. Holm's method generally provides greater power than the Bonferroni correction.


### Hochberg Procedure:

The Hochberg procedure is a powerful method that performs well for positively correlated test statistics. It is well-suited for epidemiology data, where positive dependency between infection events is likely. The Hochberg Procedure generally provides greater power than Holm's method and the Bonferroni Correction.

Pairwise comparisons were carried out with Fisher's Exact Test and p-values were simulated by a permutation test to reduce computation time. The results are shown in table 3.

```{r, echo = FALSE}
suppressPackageStartupMessages(library(gt))
suppressPackageStartupMessages(library(rstatix))

fixed <- read.csv("Variable_Probability_Results.csv")
variable <- read.csv("Fixed_Probability_Results.csv")

fixed_prob <- colSums(fixed)
var_prob <- colSums(variable)

p_vals <- c()

n <- c(1000, 1000)

for (i in 1:length(fixed_prob)) {
  sums <- matrix(c(fixed_prob[i], nrow(fixed) - fixed_prob[i], var_prob[i], nrow(variable) - var_prob[i]), nrow = 2)
  p_vals <- c(p_vals, fisher_test(sums, alternative = "two.sided", conf.level = 0.95, simulate.p.value = TRUE)$p)
}

p_bonf <- p.adjust(p_vals, method = "bonferroni")
p_holm <- p.adjust(p_vals, method = "holm")
p_hochberg <- p.adjust(p_vals, method = "hochberg")

rej_bonf <- as.integer(sum(p_bonf < 0.05))
rej_holm <- as.integer(sum(p_holm < 0.05))
rej_hochberg <- as.integer(sum(p_hochberg < 0.05))

bonf <- c(rej_bonf, round(rej_bonf/751, 5))
holm <- c(rej_holm, round(rej_holm/751, 5))
hochberg <- c(rej_hochberg, round(rej_hochberg/751, 5))

df <- data.frame(Bonferroni = bonf, Holm = holm, Hochberg = hochberg)

new_names <- c("Row Label", colnames(df))

df <- as.data.frame(t(rbind(colnames(df), df)))

colnames(df) <- c("Method", "Number of Rejections", "Proportion of Rejections")

df %>% gt() %>% tab_header(title = "Table 3: Number of rejections for each method")
```

The null hypothesis was rejected 24 times for all multiple comparison adjustments, which suggests that the constant transmission probability assumption is invalid. It is worth noting that the Bonferroni Correction is extremely conservative for 751 comparisons, therefore this result provides strong evidence of a difference between the algorithms. @alger2020scientific promotes the use of multiple experiment designs to increase the reliability of a conclusion. Consequently, we use a goodness-of-fit test to support these results.

### Goodness-of-fit test

In this section, we test the constant transmission probability assumption by a likelihood ratio test. It should be noted that the hypothesis being tested is slightly different. Whereas we previously tested for pairwise differences in likelihood of infection, the likelihood ratio test checks for an association between the two variables (Node and Algorithm). The likelihood ratio test is numerically equivalent to a test for interaction in a loglinear model. First, we calculate the total number of times each node is infected. By doing this for both algorithms, we obtain a contingency table of the form:

\begin{table}[hbtp]
  \begin{tabular}{lccccc}
  Node & 1 & 2 & 3 & 4 & ... \\
  
  \hline
  
  Algorithm 1 (Constant Transmission) & $n_{11}$ & $n_{12}$ & $n_{13}$ & $n_{14}$ & ... \\
  
  \hline
  
  Algorithm 2 (Varying Transmission) & $n_{21}$ & $n_{22}$ & $n_{23}$ & $n_{24}$ & ... \\
  
  \hline
  
  \end{tabular}
\end{table}

Where $n_{ij}$ is the number of times node j is infected for algorithm i. Denote by $n_{i+}$ the i'th row sum, and similarly by $n_{+j}$ the j'th column sum. Under the assumption of independence between algorithm and node, the expected cell count, $E_{ij}$, is given by:

$$E_{ij} = \frac{n_{i+}n_{+j}}{n}$$
where $n = \sum_{i}{n_{i+}}$. The likelihood ratio test statistic 

\begin{equation}
\label{eq:chi_square}
G = 2\sum_{ij}{n_{ij} \log \left(\frac{n_{ij}}{E_{ij}} \right)}
\end{equation}

asymptotically converges to a $\chi^{2}$ distribution with $(I - 1)(J - 1)$ degrees of freedom, where I and J are the number of rows and columns respectively in the contingency table.

```{r, echo = FALSE, messages = FALSE}
suppressPackageStartupMessages(library(broom))
library(knitr)

suppressPackageStartupMessages(library(DescTools))

table <- rbind(matrix(colSums(fixed), nrow = 1), matrix(colSums(variable), nrow = 1))

options(digits = 22)

result <- GTest(table)

output <- tidy(result) %>% mutate(p.value = ifelse(p.value == 0, "<1e-16", formatC(p.value, format = "e", digits = 4)), statistic = round(statistic, 4))
options(digits = 4) 

kable(output, caption = "Likelihood Ratio Test Results", label = "likeRatio", digits = 20)
```

The likelihood ratio test (Table \@ref(likeRatio)) corroborates the conclusion that the two algorithms produce different outcomes, with $p-value < 1 \times 10^{-16}$. This indicates potential limitations of current research, and highlights critical gaps regarding ways for researchers to ensure that results of epidemic simulations are broadly relevant.

# Future Research

In this study, the usefulness of temporal centrality measures were demonstrated for the CovidCard dataset. Further research is needed to demonstrate the usefulness of the methods presented for other datasets, and other fields of research (for instance, transportation networks, temporal ecological interactions, computer networks etc.). Simulations were used as a baseline to compare centrality measures. Future research could extend this by comparing centrality metrics directly to simulation methods. This study used a logistic regression model to estimate transmission probabilities, however coefficients could not be accurately estimated due to data limitations. A comprehensive methodology for estimating transmission probabilities is needed, with infection data, RSSI indicators and clinical knowledge used to estimate parameters of a statistical model.

# References


